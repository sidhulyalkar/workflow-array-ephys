{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15bdef0d-bd52-49e6-87a0-d569006149a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Export workflow to Neurodata Without Borders file and upload to DANDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5b737",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's change directories to find the `dj_local_conf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f49466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# change to the upper level folder to detect dj_local_conf.json\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be working with long tables, so we'll make visualization easier with a limit\n",
    "import datajoint as dj\n",
    "dj.config['display.limit']=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2161a4e",
   "metadata": {},
   "source": [
    "CodeBook users should also set a couple additional config parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c423ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_as_prefix = dj.config[\"database.user\"] + \"_\"\n",
    "if not dj.config['custom']:\n",
    "    dj.config['custom'] = {}\n",
    "dj.config['custom'].update({\n",
    "        \"ephys_mode\": \"no-curation\",\n",
    "        \"database.prefix\": username_as_prefix,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2c6ae-b8cd-47b8-af38-812f65032933",
   "metadata": {},
   "source": [
    "If you haven't already populated the `lab`, `subject`, `session`, `probe`, and `ephys` schemas, please do so now with [04-automate](./04-automate-optional.ipynb). Note: exporting `ephys` data is currently only supported on the `no_curation` schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cef246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting cbroz@dss-db.datajoint.io:3306\n"
     ]
    }
   ],
   "source": [
    "from workflow_array_ephys.pipeline import lab, subject, session, probe, ephys\n",
    "from workflow_array_ephys.export import (element_lab_to_nwb_dict, subject_to_nwb, \n",
    "                                         session_to_nwb, ecephys_session_to_nwb, \n",
    "                                         write_nwb)\n",
    "from element_interface.dandi import upload_to_dandi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd4a7c",
   "metadata": {},
   "source": [
    "## Export to NWB\n",
    "\n",
    "We'll use the following keys to demonstrate export functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cebd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_key={\"lab\": \"LabA\"}\n",
    "protocol_key={\"protocol\": \"ProtA\"}\n",
    "project_key={\"project\": \"ProjA\"}\n",
    "session_key={\"subject\": \"subject5\",\n",
    "             \"session_datetime\": \"2018-07-03 20:32:28\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d028a",
   "metadata": {},
   "source": [
    "### Upstream Elements\n",
    "\n",
    "If you plan to use all upstream Elements, you can skip to the following section. To combine with other schemas, the following functions may be helpful.\n",
    "\n",
    "- **Element Lab** `element_lab_to_nwb_dict` exports NWB-relevant items to `dict` format.\n",
    "- **Element Animal** `subject_to_nwb` returns an NWB file with subject information.\n",
    "- **Element Session** `session_to_nwb` returns an NWB file with subject and session information.\n",
    "\n",
    "Note: `pynwb` will display a warning regarding timezone information - datetime fields are assumed to be in local time, and will be converted to UTC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you don't already have data in the Element Lab\n",
    "lab.Lab.insert1(\n",
    "    {\n",
    "        **lab_key,\n",
    "        \"lab_name\": \"LabA\",\n",
    "        \"institution\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"time_zone\": \"UTC+0\",\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "lab.ProtocolType.insert1({\"protocol_type\": \"A\"})\n",
    "lab.Protocol.insert1({**protocol_key, \"protocol_type\": \"A\"}, skip_duplicates=True)\n",
    "lab.Project.insert1(project_key, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lab:\\n')\n",
    "print(element_lab_to_nwb_dict(lab_key=lab_key, protocol_key=protocol_key, \n",
    "                              project_key=project_key))\n",
    "print('\\nAnimal:\\n')\n",
    "print(subject_to_nwb(session_key=session_key))\n",
    "print('\\nSession:\\n')\n",
    "print(session_to_nwb(session_key=session_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fba81",
   "metadata": {},
   "source": [
    "### Element Array Electrophysiology\n",
    "\n",
    "`ecephys_session_to_nwb` provides a full export mechanism, returning an NWB file with raw data, spikes, and LFP. Optional arguments determine which pieces are exported. For demonstration purposes, we recommend limiting `end_frame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2f913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ecephys_session_to_nwb in module element_array_ephys.export.nwb.nwb:\n",
      "\n",
      "ecephys_session_to_nwb(session_key, raw=True, spikes=True, lfp='source', end_frame=None, lab_key=None, project_key=None, protocol_key=None, nwbfile_kwargs=None)\n",
      "    Main function for converting ephys data to NWB\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    session_key: dict\n",
      "    raw: bool\n",
      "        Whether to include the raw data from source. SpikeGLX and OpenEphys are supported\n",
      "    spikes: bool\n",
      "        Whether to include CuratedClustering\n",
      "    lfp:\n",
      "        \"dj\" - read LFP data from ephys.LFP\n",
      "        \"source\" - read LFP data from source (SpikeGLX supported)\n",
      "        False - do not convert LFP\n",
      "    end_frame: int, optional\n",
      "        Used to create small test conversions where large datasets are truncated.\n",
      "    lab_key, project_key, and protocol_key: dictionaries used to look up optional additional metadata\n",
      "    nwbfile_kwargs: dict, optional\n",
      "        - If element-session is not being used, this argument is required and must be a dictionary containing\n",
      "          'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime),\n",
      "          the minimal data for instantiating an NWBFile object.\n",
      "    \n",
      "        - If element-session is being used, this argument can optionally be used to add over overwrite NWBFile fields.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ecephys_session_to_nwb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edf9615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cb/miniconda3/envs/ele/lib/python3.8/site-packages/pynwb/file.py:1037: UserWarning: Date is missing timezone information. Updating to local timezone.\n",
      "  warn(\"Date is missing timezone information. Updating to local timezone.\")\n",
      "creating units table for paramset 0: 100%|██████████| 499/499 [00:41<00:00, 12.11it/s]\n"
     ]
    }
   ],
   "source": [
    "nwbfile = ecephys_session_to_nwb(session_key=session_key,\n",
    "                                 raw=True,\n",
    "                                 spikes=True,\n",
    "                                 lfp=\"dj\",\n",
    "                                 end_frame=100,\n",
    "                                 lab_key=lab_key,\n",
    "                                 project_key=project_key,\n",
    "                                 protocol_key=protocol_key,\n",
    "                                 nwbfile_kwargs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1131e149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root pynwb.file.NWBFile at 0x140297891486016\n",
       "Fields:\n",
       "  acquisition: {\n",
       "    ElectricalSeries1 <class 'pynwb.ecephys.ElectricalSeries'>,\n",
       "    ElectricalSeries2 <class 'pynwb.ecephys.ElectricalSeries'>\n",
       "  }\n",
       "  devices: {\n",
       "    262716621 <class 'pynwb.device.Device'>,\n",
       "    714000838 <class 'pynwb.device.Device'>\n",
       "  }\n",
       "  electrode_groups: {\n",
       "    probe262716621_shank0 <class 'pynwb.ecephys.ElectrodeGroup'>,\n",
       "    probe714000838_shank0 <class 'pynwb.ecephys.ElectrodeGroup'>\n",
       "  }\n",
       "  electrodes: electrodes <class 'hdmf.common.table.DynamicTable'>\n",
       "  experiment_description: Example project to populate element-lab\n",
       "  experimenter: ['User1']\n",
       "  file_create_date: [datetime.datetime(2022, 5, 31, 15, 47, 41, 270996, tzinfo=tzlocal())]\n",
       "  identifier: 172f2d3b-44c1-4ae1-8785-2d20d3df3db1\n",
       "  institution: Example Uni\n",
       "  keywords: ['Example' 'Study']\n",
       "  lab: The Example Lab\n",
       "  notes: Protocol for managing data ingestion\n",
       "  processing: {\n",
       "    ecephys <class 'pynwb.base.ProcessingModule'>\n",
       "  }\n",
       "  protocol: ProtA\n",
       "  related_publications: ['arXiv:1807.11104' 'arXiv:1807.11104v1']\n",
       "  session_description: Successful data collection\n",
       "  session_id: subject5_2018-07-03T20:32:28\n",
       "  session_start_time: 2018-07-04 01:32:28+00:00\n",
       "  subject: subject pynwb.file.Subject at 0x140297891485200\n",
       "Fields:\n",
       "  date_of_birth: 2020-01-01 00:00:00-06:00\n",
       "  description: {\"subject\": \"subject5\", \"sex\": \"F\", \"subject_birth_date\": \"2020-01-01\", \"subject_description\": \"rich\", \"line\": null, \"strain\": null, \"source\": null}\n",
       "  sex: F\n",
       "  subject_id: subject5\n",
       "\n",
       "  timestamps_reference_time: 2018-07-04 01:32:28+00:00\n",
       "  units: units <class 'pynwb.misc.Units'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6393fc",
   "metadata": {},
   "source": [
    "`write_nwb` can then be used to write this file to disk. The following cell will include a timestamp in the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "my_path = \"./\"\n",
    "my_path = f\"/home/{dj.config['database.user']}/\" # for codebook users\n",
    "write_nwb(nwbfile, my_path+time.strftime(\"_test_%Y%m%d-%H%M%S.nwb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717baf8",
   "metadata": {},
   "source": [
    "## DANDI Upload\n",
    "\n",
    "`element-interface.dandi` includes the `upload_to_dandi` utility to support direct uploads. For more information, see [DANDI documentation](https://www.dandiarchive.org/handbook/10_using_dandi/).\n",
    "\n",
    "In order to upload, you'll need...\n",
    "1. A DANDI account\n",
    "2. A `DANDI_API_KEY`\n",
    "3. A `dandiset_id`\n",
    "\n",
    "These values can be added to your `dj.config` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['custom']['dandiset_id']=\"<six digits as string>\" \n",
    "dj.config['custom']['dandi.api']=\"<40-character alphanumeric string>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f65d15",
   "metadata": {},
   "source": [
    "This would facilitate routine updating of your dandiset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dbce9f-825e-49a4-b49f-58b406873430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH                 SIZE     DONE            DONE% CHECKSUM STATUS          MESSAGE   \n",
      "dandiset.yaml                                                done            updated   \n",
      "test1.nwb            109.8 MB 109.8 MB         100%    ok    done                      \n",
      "Summary:             109.8 MB 109.8 MB                       2 done          1 updated \n",
      "                              100.00%                                                  \n",
      "Usage: dandi [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  A client to support interactions with DANDI archive\n",
      "  (http://dandiarchive.org).\n",
      "\n",
      "  To see help for a specific command, run\n",
      "\n",
      "      dandi COMMAND --help\n",
      "\n",
      "  e.g. dandi upload --help\n",
      "\n",
      "Options:\n",
      "  --version\n",
      "  -l, --log-level [DEBUG|INFO|WARNING|ERROR|CRITICAL]\n",
      "                                  Log level (case insensitive).  May be\n",
      "                                  specified as an integer.  [default: INFO]\n",
      "  --pdb                           Fall into pdb if errors out\n",
      "  --help                          Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete            Delete dandisets and assets from the server.\n",
      "  digest            Calculate file digests\n",
      "  download          Download a file or entire folder from DANDI.\n",
      "  instances         List known Dandi Archive instances that the CLI can...\n",
      "  ls                List .nwb files and dandisets metadata.\n",
      "  organize          (Re)organize files according to the metadata.\n",
      "  shell-completion  Emit shell script for enabling command completion.\n",
      "  upload            Upload Dandiset files to DANDI Archive.\n",
      "  validate          Validate files for NWB and DANDI compliance.\n",
      "  validate-bids     Validate BIDS paths.\n",
      "Usage: dandi [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  A client to support interactions with DANDI archive\n",
      "  (http://dandiarchive.org).\n",
      "\n",
      "  To see help for a specific command, run\n",
      "\n",
      "      dandi COMMAND --help\n",
      "\n",
      "  e.g. dandi upload --help\n",
      "\n",
      "Options:\n",
      "  --version\n",
      "  -l, --log-level [DEBUG|INFO|WARNING|ERROR|CRITICAL]\n",
      "                                  Log level (case insensitive).  May be\n",
      "                                  specified as an integer.  [default: INFO]\n",
      "  --pdb                           Fall into pdb if errors out\n",
      "  --help                          Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete            Delete dandisets and assets from the server.\n",
      "  digest            Calculate file digests\n",
      "  download          Download a file or entire folder from DANDI.\n",
      "  instances         List known Dandi Archive instances that the CLI can...\n",
      "  ls                List .nwb files and dandisets metadata.\n",
      "  organize          (Re)organize files according to the metadata.\n",
      "  shell-completion  Emit shell script for enabling command completion.\n",
      "  upload            Upload Dandiset files to DANDI Archive.\n",
      "  validate          Validate files for NWB and DANDI compliance.\n",
      "  validate-bids     Validate BIDS paths.\n",
      "work_dir: ./temp_nwb/\n",
      "data_dir: ./temp_nwb/\n",
      "dand_dir: ./temp_nwb/200178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pynwb validation errors for /Users/cb/Documents/dev/workflow-array-ephys/temp_nwb/200178/test1.nwb: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH                 SIZE     ERRORS UPLOAD STATUS           MESSAGE                  \n",
      "test1.nwb            109.8 MB   0           skipped          file exists              \n",
      "dandiset.yaml        2.0 kB                 skipped          should be edited online  \n",
      "Summary:             109.8 MB               2 skipped        1 file exists            \n",
      "                                                             1 should be edited online\n"
     ]
    }
   ],
   "source": [
    "upload_to_dandi(\n",
    "    data_directory=\"./temp_nwb/\",\n",
    "    dandiset_id=dj.config['custom']['dandiset_id'],\n",
    "    staging=True,\n",
    "    working_directory=\"./temp_nwb/\",\n",
    "    api_key=dj.config['custom']['dandi.api'],\n",
    "    sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd56de-ecf3-469b-8aed-e2484402bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py_scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ele')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d00c4ad21a7027bf1726d6ae3a9a6ef39c8838928eca5a3d5f51f3eb68720410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
